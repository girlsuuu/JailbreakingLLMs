Traceback (most recent call last):
  File "/home/vivi/Jianli_work/JailbreakingLLMs/main.py", line 217, in <module>
    main(args)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/main.py", line 40, in main
    extracted_attack_list = attackLM.get_attack(convs_list, processed_response_list)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 284, in get_attack
    valid_outputs, new_adv_prompts = self._generate_attack(processed_convs_list, init_message)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 238, in _generate_attack
    outputs_list = self.model.batched_generate(convs_subset,
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 124, in batched_generate
    outputs = self.model.generate(full_prompt, sampling_params)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/utils.py", line 1131, in inner
    return fn(*args, **kwargs)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 457, in generate
    self._validate_and_add_requests(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1317, in _validate_and_add_requests
    self._add_request(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1335, in _add_request
    self.llm_engine.add_request(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 186, in add_request
    request = self.processor.process_inputs(request_id, prompt, params,
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 209, in process_inputs
    self._validate_model_inputs(processed_inputs, lora_request)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 308, in _validate_model_inputs
    raise ValueError(
ValueError: Prompt length of 1244 is longer than the maximum model length of 1024.
Traceback (most recent call last):
  File "/home/vivi/Jianli_work/JailbreakingLLMs/main.py", line 217, in <module>
    main(args)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/main.py", line 40, in main
    extracted_attack_list = attackLM.get_attack(convs_list, processed_response_list)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 284, in get_attack
    valid_outputs, new_adv_prompts = self._generate_attack(processed_convs_list, init_message)
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 238, in _generate_attack
    outputs_list = self.model.batched_generate(convs_subset,
  File "/home/vivi/Jianli_work/JailbreakingLLMs/conversers.py", line 124, in batched_generate
    outputs = self.model.generate(full_prompt, sampling_params)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/utils.py", line 1131, in inner
    return fn(*args, **kwargs)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 457, in generate
    self._validate_and_add_requests(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1317, in _validate_and_add_requests
    self._add_request(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1335, in _add_request
    self.llm_engine.add_request(
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 186, in add_request
    request = self.processor.process_inputs(request_id, prompt, params,
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 209, in process_inputs
    self._validate_model_inputs(processed_inputs, lora_request)
  File "/home/vivi/.conda/envs/pair/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 308, in _validate_model_inputs
    raise ValueError(
ValueError: Prompt length of 1244 is longer than the maximum model length of 1024.
